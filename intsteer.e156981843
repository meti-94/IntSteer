The tokenizer you are loading from '/g/data/hn98/Mehdi/hf_home/hub/gemma-2-9b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 208.58it/s]
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
  0%|          | 0/16 [00:00<?, ?it/s]  0%|          | 0/16 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 488, in <module>
    result, graph_data = addition_steer(model, steer, hp, path, method='ActSteer')
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 252, in addition_steer
    texts, storage = steer_model_addition(model, steer, hp, default_prompt, scale=scale, n_samples=256)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/ft_effects/utils.py", line 284, in steer_model_addition
    before_difference = kl_process(storage['before'], model, steered_probs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/ft_effects/utils.py", line 172, in kl_process
    dist = dist.reshape(256, 30, 2304)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: shape '[256, 30, 2304]' is invalid for input of size 27525120
The tokenizer you are loading from '/g/data/hn98/Mehdi/hf_home/hub/gemma-2-2b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 147.60it/s]
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
  0%|          | 0/16 [00:00<?, ?it/s]  0%|          | 0/16 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 488, in <module>
    result, graph_data = addition_steer(model, steer, hp, path, method='ActSteer')
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 252, in addition_steer
    texts, storage = steer_model_addition(model, steer, hp, default_prompt, scale=scale, n_samples=256)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/ft_effects/utils.py", line 295, in steer_model_addition
    storage['bf'][L] = [t.squeeze().tolist() for t in storage['bf'][L]]
                                                      ~~~~~~~~~~~~~^^^
IndexError: list index out of range

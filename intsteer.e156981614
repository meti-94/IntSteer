The tokenizer you are loading from '/g/data/hn98/Mehdi/hf_home/hub/gemma-2-9b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 207.25it/s]
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
  0%|          | 0/16 [00:00<?, ?it/s]  0%|          | 0/16 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 488, in <module>
    result, graph_data = addition_steer(model, steer, hp, path, method='ActSteer')
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 252, in addition_steer
    texts, storage = steer_model_addition(model, steer, hp, default_prompt, scale=scale, n_samples=256)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/ft_effects/utils.py", line 284, in steer_model_addition
    before_difference = kl_process(storage['before'], model, steered_probs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/ft_effects/utils.py", line 168, in kl_process
    dist = torch.stack(storage)
           ^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects a non-empty TensorList
The tokenizer you are loading from '/g/data/hn98/Mehdi/hf_home/hub/gemma-2-2b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 62.18it/s]
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
  0%|          | 0/16 [00:00<?, ?it/s]  0%|          | 0/16 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 488, in <module>
    result, graph_data = addition_steer(model, steer, hp, path, method='ActSteer')
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/baselines/analysis.py", line 252, in addition_steer
    texts, storage = steer_model_addition(model, steer, hp, default_prompt, scale=scale, n_samples=256)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/ft_effects/utils.py", line 284, in steer_model_addition
    before_difference = kl_process(storage['before'], model, steered_probs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/hn98/Mehdi/test/IntSteer/src/sae_ts/ft_effects/utils.py", line 168, in kl_process
    dist = torch.stack(storage)
           ^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects a non-empty TensorList
